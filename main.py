import sys
import os
import torch
import argparse
import json
import time
import torch.nn as nn
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

# å°è¯•å¯¼å…¥ matplotlibï¼Œå¦‚æœå¤±è´¥åˆ™ gracefully degrade
try:
    import matplotlib.pyplot as plt
    import numpy as np
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

import logging

# ä¾ç„¶ä¿ç•™ HF_ENDPOINTï¼Œä»¥é˜²ä¸‡ä¸€æœªæ¥éœ€è¦æ‰©å±•
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

sys.path.append(os.getcwd())

from core.engine import SearchEngine
from core.compressor import Compressor
from core.evaluator import Evaluator
from utils.data_loader import get_calib_dataset
from utils.plotter import generate_performance_plot, generate_search_history_plot, generate_interactive_search_history_plot
from methods.quantization.fp16 import FP16Quantization
from methods.quantization.int8_sq import INT8SQQuantization
from methods.pruning.random import RandomPruning
from methods.pruning.l2 import L2StructuredPruning
from methods.retraining.finetuning import CausalLMFinetuning

# é…ç½®å…¨å±€æ—¥å¿—
logger = logging.getLogger(__name__)

def setup_logging(run_dir):
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    """
    log_path = os.path.join(run_dir, "run.log")
    
    # è®¾ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    logger.info(f"Logging initialized. Log file: {log_path}")

def parse_args():
    parser = argparse.ArgumentParser(description="AutoLLM-Compressor: è‡ªåŠ¨åŒ–å¤§æ¨¡å‹å‹ç¼©å·¥å…·")
    
    default_model_path = os.path.abspath("./models/Llama-2-7b-hf")
    
    parser.add_argument("--model_path", type=str, default=default_model_path, help="æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„")
    parser.add_argument("--strategy", type=str, default="bayesian", choices=["grid", "random", "bayesian"], help="æœç´¢ç­–ç•¥ (é»˜è®¤: bayesian)")
    parser.add_argument("--n_trials", type=int, default=30, help="è´å¶æ–¯æœç´¢çš„å°è¯•æ¬¡æ•° (é»˜è®¤: 30)")
    parser.add_argument("--data_samples", type=int, default=10, help="æ ¡å‡†æ•°æ®æ ·æœ¬æ•°é‡")
    parser.add_argument("--data_path", type=str, default=None, help="å¤–éƒ¨æ•°æ®é›†è·¯å¾„ï¼ˆå¦‚ wikitext2 çš„ test.txtï¼‰")
    parser.add_argument("--save_to_local", action="store_true", help="æ˜¯å¦ä¿å­˜å‹ç¼©åçš„æ¨¡å‹")
    
    # æ–°å¢: æ§åˆ¶æ˜¯å¦åœ¨æ··åˆæ¨¡å¼ä¸‹å¯ç”¨å†è®­ç»ƒ
    parser.add_argument("--retrain", type=lambda x: (str(x).lower() == 'true'), default=True, help="æ··åˆæ¨¡å¼ä¸‹æ˜¯å¦å¯ç”¨å†è®­ç»ƒ (True/False), é»˜è®¤ True")
    
    # ä¿®æ”¹: æ˜¾å¼æ”¯æŒ --cpu å’Œ --gpuï¼Œä¸”é»˜è®¤ä½¿ç”¨ cpu (é™¤éæœ‰ gpu ä¸”æ²¡æŒ‡å®š cpu)
    # ä¸ºäº†å®ç°â€œé»˜è®¤ CPUâ€ä½†åˆå…è®¸â€œè‡ªåŠ¨æ£€æµ‹â€ï¼Œæˆ‘ä»¬ä½¿ç”¨äº’æ–¥ç»„
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨ CPU")
    group.add_argument("--gpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨ GPU (CUDA)")
    
    return parser.parse_args()

def get_device(args):
    """
    æ ¹æ®å‚æ•°å†³å®šä½¿ç”¨å“ªä¸ªè®¾å¤‡
    """
    if args.gpu:
        if torch.cuda.is_available():
            return "cuda"
        else:
            print("âš ï¸ Warning: GPU requested but CUDA is not available. Falling back to CPU.")
            return "cpu"
    elif args.cpu:
        return "cpu"
    else:
        # é»˜è®¤è¡Œä¸ºï¼šä¿®æ”¹ä¸ºé»˜è®¤ CPU (æ ¹æ®ç”¨æˆ·éœ€æ±‚)ï¼Œæˆ–è€…ä¿æŒè‡ªåŠ¨æ£€æµ‹
        # ç”¨æˆ·éœ€æ±‚ï¼šé»˜è®¤ CPU
        return "cpu"

def save_results(args, original_ppl, final_ppl, best_config, final_model, tokenizer, run_dir, picture_dir):
    if args.save_to_local and final_model and tokenizer:
        model_save_dir = os.path.join(run_dir, "model")
        logger.info(f"Saving compressed model to: {model_save_dir}...")
        try:
            if not os.path.exists(model_save_dir):
                os.makedirs(model_save_dir)
            
            final_model.save_pretrained(model_save_dir)
            tokenizer.save_pretrained(model_save_dir)
            logger.info(f"Compressed model saved successfully.")
        except Exception as e:
            logger.error(f"Failed to save compressed model: {e}")

    search_history = best_config.get("search_history", [])

    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "model_path": args.model_path,
        "strategy": args.strategy,
        "data_samples": args.data_samples,
        "original_ppl": original_ppl,
        "final_ppl": final_ppl,
        "ppl_change": final_ppl - original_ppl,
        "best_config": best_config,
        "search_history": search_history # æ–°å¢ï¼šä¿å­˜æ‰€æœ‰æœç´¢è®°å½•
    }

    report_path = os.path.join(run_dir, "report.json")
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=4, ensure_ascii=False, default=str)
        logger.info(f"Report saved to {report_path}")
    except Exception as e:
        logger.error(f"Failed to save report: {e}")

    # 4. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if HAS_MATPLOTLIB:
        # ä¿®æ”¹: ä¿å­˜åˆ° picture ç›®å½•
        plot_path = os.path.join(picture_dir, "performance_analysis.png")
        try:
            generate_performance_plot(original_ppl, final_ppl, best_config, plot_path)
            logger.info(f"Visualization saved to: {plot_path}")
            
            # é¢å¤–ï¼šç”Ÿæˆæœç´¢å†å²æ•£ç‚¹å›¾ (Pareto Frontier)
            if search_history:
                history_plot_path = os.path.join(picture_dir, "search_space_analysis.png")
                # ç§»é™¤ target_ratio å‚æ•°
                generate_search_history_plot(search_history, original_ppl, save_path=history_plot_path)
                logger.info(f"Search Space Visualization saved to: {history_plot_path}")

                # æ–°å¢ï¼šç”Ÿæˆäº¤äº’å¼ HTML å›¾è¡¨
                interactive_plot_path = os.path.join(picture_dir, "search_space_analysis.html")
                generate_interactive_search_history_plot(search_history, original_ppl, save_path=interactive_plot_path)

                
        except Exception as e:
            logger.error(f"Failed to generate plot: {e}")
    else:
        logger.warning("Matplotlib not installed. Skipping visualization.")
        logger.warning("Tip: Run `pip install matplotlib` to enable charts.")

def load_model(model_name_or_path, device):
    print(f"Loading model from: {model_name_or_path}")
    
    if not os.path.exists(model_name_or_path):
        print(f"\nâŒ CRITICAL ERROR: Model path not found locally: {model_name_or_path}")
        print("Please download the model first (e.g., using scripts/download_model.py).")
        print("Exiting to prevent unintended network requests.")
        sys.exit(1)

    print(f"Detected local path: {model_name_or_path}")
    try:
        dtype = torch.float16 if "cuda" in device else torch.float32
        
        # å¼ºåˆ¶ local_files_only=Trueï¼Œä¸¥ç¦è”ç½‘
        # æ˜¾å­˜ä¼˜åŒ–ï¼šé¿å…åˆå§‹åŠ è½½å ç”¨è¿‡å¤š
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            torch_dtype=dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            local_files_only=True 
        )
        # ä¸è¦åœ¨è¿™é‡Œ to(device)ï¼Œè®©è°ƒç”¨è€…å†³å®šä½•æ—¶ç§»åŠ¨ï¼Œé¿å…åŒå€æ˜¾å­˜å ç”¨
        # print(f"Successfully loaded {model.__class__.__name__}")
        return model
    except Exception as e:
        print(f"\nâŒ Failed to load local model: {e}")
        sys.exit(1)

import multiprocessing

def run_worker(rank, world_size, args, run_dir, picture_dir, storage_url, study_name):
    """
    å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šç‹¬ç«‹åŠ è½½æ¨¡å‹ä¸æ•°æ®ï¼Œæ‰§è¡Œæœç´¢ä»»åŠ¡
    """
    # è®¾ç½®å½“å‰è¿›ç¨‹å¯è§çš„ GPU
    # å¦‚æœæœ‰å¤šå¼ å¡ï¼Œrank å¯¹åº” GPU ID
    if args.gpu and torch.cuda.device_count() > 1:
        # å­è¿›ç¨‹ä¸éœ€è¦é‡æ–°è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œå› ä¸ºåœ¨ Process å¯åŠ¨å‰è¿˜æ²¡åˆå§‹åŒ– CUDA
        # ä½†æ˜¯åœ¨ spawn æ¨¡å¼ä¸‹ï¼Œå­è¿›ç¨‹æ˜¯å…¨æ–°çš„ï¼Œæ‰€ä»¥éœ€è¦ç¡®ä¿å®ƒåªçœ‹åˆ°æŒ‡å®šçš„ GPU
        # æ³¨æ„ï¼šåœ¨ spawn æ¨¡å¼ä¸‹ï¼Œos.environ çš„ä¿®æ”¹ä¼šä¼ é€’ç»™å­è¿›ç¨‹ï¼Œä½†å¦‚æœåœ¨ä¸»è¿›ç¨‹æ”¹äº†ï¼Œå¯èƒ½ä¼šå½±å“å…¶ä»–ã€‚
        # æœ€å¥½çš„æ–¹å¼æ˜¯åœ¨å­è¿›ç¨‹ä¸€å¼€å§‹å°±è®¾ç½®ã€‚
        os.environ["CUDA_VISIBLE_DEVICES"] = str(rank)
        # å¼ºåˆ¶ device ä¸º cuda:0 (å› ä¸ºå¯¹å­è¿›ç¨‹æ¥è¯´ï¼Œå®ƒåªæœ‰è¿™ä¸€å¼ å¡)
        # ä¿®æ­£ï¼šå½“ CUDA_VISIBLE_DEVICES=rank æ—¶ï¼ŒPython çœ‹åˆ°çš„è®¾å¤‡ ID æ˜¯ 0
        device = "cuda:0" 
    elif args.gpu:
         # å•å¡å¤šè¿›ç¨‹æƒ…å†µï¼ˆä¸æ¨èï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ï¼‰
         # æˆ–è€…åœ¨ args.gpu ä¸” device_count==1 æ—¶ï¼Œä¹Ÿåº”è¯¥å…è®¸è¿è¡Œ
         device = get_device(args)
    else:
        # å•å¡æˆ– CPU æ¨¡å¼
        device = get_device(args)
    
    # åˆå§‹åŒ–æ—¥å¿—ï¼ˆæ¯ä¸ªè¿›ç¨‹éœ€è¦ç‹¬ç«‹çš„ logger æˆ–è€…æ˜¯å‘åŒä¸€ä¸ªæ–‡ä»¶å†™ï¼Ÿè¿™é‡Œç®€å•èµ·è§ï¼Œè®©ä¸»è¿›ç¨‹è´Ÿè´£ä¸»æ—¥å¿—ï¼Œå­è¿›ç¨‹åªè¾“å‡ºåˆ°æ§åˆ¶å°æˆ–å…±äº«æ–‡ä»¶ï¼‰
    # ç”±äºå¤šè¿›ç¨‹å†™åŒä¸€ä¸ªæ–‡ä»¶å¯èƒ½ä¼šå†²çªï¼Œè¿™é‡Œæˆ‘ä»¬ä¾èµ– setup_logging åœ¨ä¸»è¿›ç¨‹åšå¥½çš„é…ç½®ï¼ˆå¦‚æœæ˜¯ forkï¼‰ï¼Œ
    # ä½† Windows æ˜¯ spawnï¼Œæ‰€ä»¥éœ€è¦é‡æ–°é…ç½®ã€‚ä¸ºäº†é¿å…æ··ä¹±ï¼Œå­è¿›ç¨‹æ—¥å¿—åŠ å‰ç¼€ã€‚
    # ç®€å•èµ·è§ï¼Œé‡æ–°è°ƒç”¨ setup_loggingï¼Œä½†å¯èƒ½è¦æ³¨æ„æ–‡ä»¶é”ã€‚
    # æˆ‘ä»¬æš‚æ—¶è®©å­è¿›ç¨‹åªè¾“å‡ºåˆ° stdout
    
    worker_prefix = f"[Worker-{rank}] "
    print(f"{worker_prefix}Starting process on device {device}")

    # 1. åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device)
    model.to(device)
    
    # 2. åŠ è½½ Tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True, local_files_only=True)
    except Exception as e:
        print(f"{worker_prefix}Failed to load tokenizer: {e}")
        return

    # 3. å‡†å¤‡æ•°æ®
    try:
        dataset = get_calib_dataset(
            data_name="wikitext2", 
            tokenizer_name=None, 
            n_samples=args.data_samples,
            tokenizer_obj=tokenizer,
            data_path=args.data_path
        )
        dataset = [d.to(device) for d in dataset]
    except Exception as e:
        print(f"{worker_prefix}Failed to load data: {e}")
        return

    # 4. åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = Evaluator(dataset, device=device)
    
    # 5. åˆå§‹åŒ–æœç´¢å¼•æ“
    engine = SearchEngine(search_strategy=args.strategy, evaluator=evaluator)
    
    # 6. å¼€å§‹æœç´¢ (è¿æ¥åˆ°åŒä¸€ä¸ª Study)
    # è®¡ç®—æ­¤ Worker åˆ†é…åˆ°çš„ trials æ•° (å¦‚æœéœ€è¦å¹³å‡åˆ†é…ï¼Œæˆ–è€…è®© Optuna æŠ¢å å¼åˆ†é…)
    # Optuna çš„ storage æ¨¡å¼æ”¯æŒæŠ¢å å¼ï¼Œæ‰€æœ‰ worker å…±åŒå®Œæˆæ€» n_trials
    # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥è®©æ¯ä¸ª worker è·‘ n_trials / world_sizeï¼Œæˆ–è€…ç›´æ¥è®¾å®šæ€»æ•°
    # Optuna çš„ optimize æ˜¯â€œè·‘ n_trials æ¬¡â€ï¼Œå¦‚æœæ˜¯åˆ†å¸ƒå¼ï¼Œæ„å‘³ç€â€œè¿™ä¸ªè¿›ç¨‹è·‘ n_trials æ¬¡â€ã€‚
    # æˆ‘ä»¬å¸Œæœ›æ€»å…±è·‘ n_trials æ¬¡ã€‚
    # æ­£ç¡®çš„åšæ³•æ˜¯ï¼šä¸æŒ‡å®š n_trials ç»™ optimizeï¼Œæˆ–è€…åŠ¨æ€æ£€æŸ¥ã€‚
    # ä½† Optuna çš„ API optimize(n_trials=N) æ˜¯æŒ‡â€œè¿™ä¸ª worker æ‰§è¡Œ N æ¬¡â€ã€‚
    # å¦‚æœæˆ‘ä»¬è¦æ€»å…± N æ¬¡ï¼Œæœ€ç®€å•çš„åŠæ³•æ˜¯å¹³å‡åˆ†ã€‚
    
    my_trials = args.n_trials // world_size
    if rank < args.n_trials % world_size:
        my_trials += 1
        
    print(f"{worker_prefix}Will execute {my_trials} trials...")
    
    constraints = {
        "n_trials": my_trials,
        "enable_retrain": args.retrain,
        "study_name": study_name,
        "storage": storage_url
    }
    
    try:
        # æ‰§è¡Œæœç´¢
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦è¿”å›å€¼ï¼Œå› ä¸ºä¸»è¿›ç¨‹ä¼šä» storage è¯»å–æœ€ä½³ç»“æœ
        # ä½†ä¸ºäº†å¤ç”¨ä»£ç ï¼Œsearch ä¼šè¿”å› best_config
        engine.search(model, constraints)
        print(f"{worker_prefix}Finished.")
    except Exception as e:
        print(f"{worker_prefix}Error during search: {e}")
        import traceback
        traceback.print_exc()

def main():
    args = parse_args()
    
    # 0. æå‰åˆ›å»ºç›®å½•
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    run_dir_name = f"result_{timestamp}"
    base_result_dir = "./results"
    run_dir = os.path.join(base_result_dir, run_dir_name)
    picture_dir = os.path.join(run_dir, "picture")
    
    if not os.path.exists(run_dir):
        os.makedirs(run_dir)
    if not os.path.exists(picture_dir):
        os.makedirs(picture_dir)
        
    setup_logging(run_dir)
    
    # æ£€æµ‹ GPU æ•°é‡
    gpu_count = torch.cuda.device_count()
    use_parallel = args.gpu and gpu_count > 1 and args.strategy == "bayesian"
    
    if use_parallel:
        logger.info(f"ğŸš€ Detected {gpu_count} GPUs. Enabling Parallel Bayesian Search!")
        
        # é‡Šæ”¾ä¸»è¿›ç¨‹åŠ è½½çš„æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜ï¼Œç•™ç»™å­è¿›ç¨‹ä½¿ç”¨
        torch.cuda.empty_cache()
        logger.info("Cleared main process model to free up GPU memory for workers.")
        
        # å‡†å¤‡ Optuna Storage (SQLite)
        db_path = os.path.join(run_dir, "optuna.db")
        storage_url = f"sqlite:///{db_path}"
        study_name = f"study_{timestamp}"
        
        logger.info(f"Optuna Storage: {storage_url}")
        
        # å¿…é¡»è®¾ç½® spawn å¯åŠ¨æ–¹å¼ï¼Œå¦åˆ™ CUDA åˆå§‹åŒ–ä¼šæŠ¥é”™
        # æ³¨æ„ï¼šset_start_method åªèƒ½è°ƒç”¨ä¸€æ¬¡ï¼Œè¿™é‡ŒåŠ  try-except
        try:
            multiprocessing.set_start_method("spawn", force=True)
        except RuntimeError:
            pass # å·²ç»è®¾ç½®è¿‡ä¹Ÿæ²¡å…³ç³»
        
        # å¯åŠ¨å¤šè¿›ç¨‹ Workers
        processes = []
        for rank in range(gpu_count):
            p = multiprocessing.Process(
                target=run_worker,
                args=(rank, gpu_count, args, run_dir, picture_dir, storage_url, study_name)
            )
            p.start()
            processes.append(p)
            
        # ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
        for p in processes:
            p.join()
            
        logger.info("All workers finished. Aggregating results...")
        
        # ä¸»è¿›ç¨‹åŠ è½½æœ€ä½³ç»“æœå¹¶è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        # éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹ï¼ˆåœ¨ä¸»è¿›ç¨‹è®¾å¤‡ä¸Šï¼Œé€šå¸¸æ˜¯ gpu:0ï¼‰
        device = "cuda:0"
        model = load_model(args.model_path, device)
        model.to(device)
        
        # åŠ è½½æ•°æ®å’Œè¯„ä¼°å™¨ç”¨äºæœ€ç»ˆéªŒè¯
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True, local_files_only=True)
        dataset = get_calib_dataset(
            data_name="wikitext2", n_samples=args.data_samples, 
            tokenizer_obj=tokenizer, data_path=args.data_path
        )
        dataset = [d.to(device) for d in dataset]
        evaluator = Evaluator(dataset, device=device)
        
        # ä» Storage ä¸­è¯»å–æœ€ä½³ Study
        import optuna
        try:
            study = optuna.load_study(study_name=study_name, storage=storage_url)
            logger.info(f"Best params found in study: {study.best_params}")
            logger.info(f"Best value (PPL): {study.best_value}")
        except KeyError:
            logger.error("Study not found in storage. It seems all workers failed.")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Failed to load study: {e}")
            sys.exit(1)
        
        # é‡æ„æœ€ä½³é…ç½®
        # æ³¨æ„ï¼šOptuna å­˜å‚¨çš„æ˜¯æ‰å¹³çš„ paramsï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢å› config å­—å…¸
        # è¿™æ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸º engine._search_bayesian é‡Œçš„ objective å‡½æ•°åšäº†è½¬æ¢é€»è¾‘
        # æ›´å¥½çš„åŠæ³•æ˜¯ï¼šè®© search æ–¹æ³•è¿”å› best_configï¼Œä½†è¿™åœ¨å¤šè¿›ç¨‹ä¸‹æ‹¿ä¸åˆ°ã€‚
        # æ›¿ä»£æ–¹æ¡ˆï¼šWorker å·²ç»æŠŠ best_config æ‰¾åˆ°äº†ï¼Œä½†æ²¡æ³•ä¼ å›ã€‚
        # æˆ‘ä»¬éœ€è¦é‡æ–°è§£æ best_paramsã€‚
        # æˆ–è€…ï¼Œæˆ‘ä»¬åœ¨ Engine é‡ŒæŠŠ best_config å­˜åˆ° UserAttrsï¼Ÿ
        # é‰´äº engine.py å·²ç»æœ‰äº†å¤æ‚çš„è½¬æ¢é€»è¾‘ï¼Œæˆ‘ä»¬åœ¨ä¸»è¿›ç¨‹é‡Œåªèƒ½æ‰‹åŠ¨å¤ç°é‚£ä¸ªè½¬æ¢ï¼Œæˆ–è€…...
        # ç®€å•æ–¹æ¡ˆï¼šç›´æ¥ç”¨ best_params é‡Œçš„ä¿¡æ¯æ„é€  configã€‚
        # ç”±äºå‚æ•°å±•å¹³äº†ï¼Œè¿™æœ‰ç‚¹å¤æ‚ã€‚
        # è®©æˆ‘ä»¬ä¿®æ”¹ Engineï¼ŒæŠŠ best_config åºåˆ—åŒ–å­˜åˆ° Study çš„ user_attrs é‡Œï¼Ÿ
        # ä½† Optuna çš„ user_attrs æ˜¯ trial çº§åˆ«çš„ã€‚
        # Study çº§åˆ«çš„ user_attrs å¯ä»¥ç”¨ study.set_user_attr()ã€‚
        # ä½†å¤šä¸ª worker åŒæ—¶è·‘ï¼Œè°æ¥ set best?
        # å…¶å®æˆ‘ä»¬åªéœ€è¦ trial.user_attrs["config"] = configã€‚
        # ç„¶å study.best_trial.user_attrs["config"] å°±æ˜¯æˆ‘ä»¬è¦çš„ã€‚
        
        # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ engine.pyï¼Œåœ¨ objective é‡ŒæŠŠ config å­˜å…¥ trial.user_attrs
        
        best_trial = study.best_trial
        if "config" in best_trial.user_attrs:
             best_config = best_trial.user_attrs["config"]
        else:
             # å¦‚æœ engine æ²¡æ”¹ï¼Œåªèƒ½ fallback (æˆ–è€…ç°åœ¨å»æ”¹ engine)
             logger.warning("Could not retrieve full config from trial.user_attrs. Parallel search requires engine update.")
             # è¿™é‡Œæˆ‘ä»¬å…ˆå‡è®¾ engine ä¼šæ”¹ï¼Œæˆ–è€…åœ¨è¿™é‡Œç›´æ¥ç”¨ best_params çŒœ
             best_config = {} # TODO: Fix this by updating engine.py
        
    else:
        # å•è¿›ç¨‹æ¨¡å¼ (åŸæœ‰é€»è¾‘)
        device = get_device(args)
    
        logger.info(f"=== AutoLLM-Compressor Project Started ===")
        logger.info(f"Arguments: {vars(args)}")
        logger.info(f"Using Device: {device}")
        
        # 2. åŠ è½½æ¨¡å‹ (ä¸¥æ ¼æœ¬åœ°æ¨¡å¼)
        model = load_model(args.model_path, device)
        model.to(device)

        # 3. åŠ è½½ Tokenizer (ä¸¥æ ¼æœ¬åœ°æ¨¡å¼)
        logger.info(f"Loading tokenizer from: {args.model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True, local_files_only=True)
        except Exception as e:
            logger.error(f"Failed to load local tokenizer from {args.model_path}")
            logger.error(f"Error: {e}")
            sys.exit(1)

        # 4. å‡†å¤‡æ•°æ®
        logger.info("Preparing calibration data...")
        try:
            dataset = get_calib_dataset(
                data_name="wikitext2", 
                tokenizer_name=None, 
                n_samples=args.data_samples,
                tokenizer_obj=tokenizer,
                data_path=args.data_path
            )
        except FileNotFoundError as e:
            logger.critical(f"{e}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Unexpected error loading data: {e}")
            sys.exit(1)
        
        dataset = [d.to(device) for d in dataset]

        # 5. åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = Evaluator(dataset, device=device)
        
        # 6. è¯„ä¼°åŸå§‹æ¨¡å‹
        logger.info("--- Evaluating Original Model ---")
        try:
            original_ppl = evaluator.evaluate_perplexity(model)
            logger.info(f"Original PPL: {original_ppl:.4f}")
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            original_ppl = float('inf')
        
        # 7. åˆå§‹åŒ–æœç´¢å¼•æ“
        engine = SearchEngine(search_strategy=args.strategy, evaluator=evaluator)
        
        # 8. å¼€å§‹è‡ªåŠ¨æœç´¢
        logger.info("--- Starting Automatic Search ---")
        
        constraints = {
            "n_trials": args.n_trials,
            "enable_retrain": args.retrain
        }
        best_config = engine.search(model, constraints)

    # === å…¬å…±ç»“æŸéƒ¨åˆ† (åº”ç”¨æœ€ä½³é…ç½®å¹¶ä¿å­˜) ===
    # æ³¨æ„ï¼šå¹¶è¡Œæ¨¡å¼ä¸‹ï¼Œmodel, tokenizer, original_ppl, best_config éƒ½éœ€è¦åœ¨ if/else å—ä¸­å‡†å¤‡å¥½
    # åœ¨å¹¶è¡Œæ¨¡å¼çš„ if å—é‡Œï¼Œæˆ‘ä»¬éœ€è¦è¡¥å…¨ original_ppl å’Œ best_config çš„è·å–
    
    if use_parallel:
        # å¹¶è¡Œæ¨¡å¼ä¸‹è¡¥å…¨ original_ppl
        logger.info("--- Evaluating Original Model (Final Check) ---")
        original_ppl = evaluator.evaluate_perplexity(model)
        
        # è¡¥å…¨ best_config (ä¾èµ– engine æ›´æ–°)
        # å¦‚æœ engine æ²¡å­˜ user_attrsï¼Œè¿™é‡Œä¼šå‡ºé”™ã€‚æ‰€ä»¥å¿…é¡»æ›´æ–° engine.py
        pass 

    logger.info(f"Best Configuration Found: {best_config}")
    
    # 9. ä½¿ç”¨æœ€ä½³é…ç½®æ‰§è¡Œæœ€ç»ˆå‹ç¼©
    logger.info("--- Applying Best Compression ---")
    compressor = Compressor()
    final_model = compressor.run(model, best_config)
    
    # 10. æœ€ç»ˆè¯„ä¼°
    logger.info("--- Evaluating Compressed Model ---")
    final_ppl = evaluator.evaluate_perplexity(final_model)
    
    logger.info("=== Final Report ===")
    logger.info(f"Original PPL: {original_ppl:.4f}")
    logger.info(f"Final PPL:    {final_ppl:.4f}")
    logger.info(f"Best Config:  {best_config}")

    # 11. ä¿å­˜ç»“æœ
    save_results(args, original_ppl, final_ppl, best_config, final_model, tokenizer, run_dir, picture_dir)

if __name__ == "__main__":
    main()
